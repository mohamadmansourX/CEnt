{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cote', 'dice', 'growing_spheres']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from carla.data.api import data\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.is_available = lambda : False\n",
    "import yaml\n",
    "from seed_env import seed_my_session\n",
    "from typing import Dict, List\n",
    "from cent.data_specific import DataModels\n",
    "from carla import Benchmark\n",
    "import pandas as pd\n",
    "from carla.recourse_methods import (\n",
    "    CCHVAE,\n",
    "    CEM,\n",
    "    CRUD,\n",
    "    FOCUS,\n",
    "    ActionableRecourse,\n",
    "    CausalRecourse,\n",
    "    Clue,\n",
    "    Dice,\n",
    "    Face,\n",
    "    FeatureTweak,\n",
    "    GrowingSpheres,\n",
    "    Revise,\n",
    "    Wachter,\n",
    ")\n",
    "from carla.recourse_methods.catalog.causal_recourse import constraints, samplers\n",
    "import carla.evaluation.catalog as evaluation_catalog\n",
    "from cent.method import CEnt\n",
    "from vae_benchmark import VAEBenchmark\n",
    "\n",
    "seed_my_session()\n",
    "def load_setup() -> Dict:\n",
    "    with open(\"experimental_setup.yaml\", \"r\") as f:\n",
    "        setup_catalog = yaml.safe_load(f)\n",
    "    return setup_catalog[\"recourse_methods\"]\n",
    "\n",
    "def print_conf(conf, d=4, d_iter=5):\n",
    "    for k, v in conf.items():\n",
    "        if isinstance(v, dict):\n",
    "            print(\"{}{} : \".format(d * \" \", str(k)))\n",
    "            print_conf(v, d + d_iter)\n",
    "        elif isinstance(v, list) and len(v) >= 1 and isinstance(v[0], dict):\n",
    "            print(\"{}{} : \".format(d * \" \", str(k)))\n",
    "            for value in v:\n",
    "                print_conf(value, d + d_iter)\n",
    "        else:\n",
    "            print(\"{}{} : {}\".format(d * \" \", k, v))\n",
    "        \n",
    "def get_resource_supported_backend(recourse_method, supported_backend_dict):\n",
    "    '''\n",
    "    Search in supported_backend for the recourse_method to know what backend is supported\n",
    "    '''\n",
    "    # supported_backend contains 'pytorch', 'tensorflow', 'xgboost', 'sklearn'\n",
    "    # Check what backend contains the recourse_method\n",
    "    suuported_backs = []\n",
    "    for backend in supported_backend_dict:\n",
    "        if recourse_method in supported_backend_dict[backend]:\n",
    "            suuported_backs.append(backend)\n",
    "    # If tensorflow and pytorch are in the list, keep only tensorflow\n",
    "    if \"tensorflow\" in suuported_backs and \"pytorch\" in suuported_backs:\n",
    "        # Remove pytorch from the list\n",
    "        suuported_backs.remove(\"pytorch\")\n",
    "    # Similarly for xgboost and sklearn, keep xgboost\n",
    "    if \"xgboost\" in suuported_backs and \"sklearn\" in suuported_backs:\n",
    "        suuported_backs.remove(\"sklearn\")\n",
    "    #TODO: Keep both, but temp return only first\n",
    "    return suuported_backs[0]\n",
    "\n",
    "def intialialize_recourse_method(method, hyperparams, mlmodel, data_models):\n",
    "    # TODO restrict data to training only\n",
    "    if method == \"cchvae\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        hyperparams[\"vae_params\"][\"layers\"] = [\n",
    "            len(mlmodel.feature_input_order)\n",
    "        ] + hyperparams[\"vae_params\"][\"layers\"]\n",
    "        return CCHVAE(mlmodel, hyperparams)\n",
    "    elif \"cem\" in method:\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        raise ValueError(\"Session Methods not supported yet\")\n",
    "        #return CEM(sess, mlmodel, hyperparams)\n",
    "    elif method == \"clue\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        return Clue(mlmodel.data, mlmodel, hyperparams)\n",
    "    elif method == \"cruds\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        # variable input layer dimension is first time here available\n",
    "        hyperparams[\"vae_params\"][\"layers\"] = [\n",
    "            len(mlmodel.feature_input_order)\n",
    "        ] + hyperparams[\"vae_params\"][\"layers\"]\n",
    "        return CRUD(mlmodel, hyperparams)\n",
    "    elif method == \"dice\":\n",
    "        return Dice(mlmodel, hyperparams)\n",
    "    elif \"face\" in method:\n",
    "        return Face(mlmodel, hyperparams)\n",
    "    elif method == \"growing_spheres\":\n",
    "        return GrowingSpheres(mlmodel)\n",
    "    elif method == \"revise\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        # variable input layer dimension is first time here available\n",
    "        hyperparams[\"vae_params\"][\"layers\"] = [\n",
    "            len(mlmodel.feature_input_order)\n",
    "        ] + hyperparams[\"vae_params\"][\"layers\"]\n",
    "        return Revise(mlmodel, mlmodel.data, hyperparams)\n",
    "    elif \"wachter\" in method:\n",
    "        return Wachter(mlmodel, hyperparams)\n",
    "    elif \"causal_recourse\" in method:\n",
    "        return CausalRecourse(mlmodel, hyperparams)\n",
    "    elif \"focus\" in method:\n",
    "        hyperparams = {'optimizer': 'adam', 'lr': 0.001, 'n_class': 2, 'n_iter': 1000, 'sigma': 1.0, 'temperature': 1.0, 'distance_weight': 0.01, 'distance_func': 'l1'}\n",
    "        return FOCUS(mlmodel, hyperparams)\n",
    "    elif \"feature_tweak\" in method:\n",
    "        return FOCUS(mlmodel, hyperparams)\n",
    "    elif \"cent\" in method:\n",
    "        min_entries_per_label = int(data_models.trainData.df.shape[0]*0.02)\n",
    "        if min_entries_per_label<900:\n",
    "            min_entries_per_label = 900\n",
    "        hpr = {\"data_name\": \"data_name\",\"n_search_samples\": 300,\"p_norm\": 1,\"step\": 0.1,\"max_iter\": 10,\"clamp\": True,\n",
    "                \"binary_cat_features\": True,\n",
    "                \"myvae_params\": {\n",
    "                    'input_dim': len(mlmodel.feature_input_order),\n",
    "                    'kld_weight': 0.00025,\n",
    "                    'layers': layers,\n",
    "                    'latent_dim': latent_dim,\n",
    "                    'hidden_activation': 'relu',\n",
    "                    'dropout': 0.2,\n",
    "                    'batch_norm': True,\n",
    "                    'batch_size': 128,\n",
    "                    'epochs': 1,\n",
    "                    'learning_rate': 0.001,\n",
    "                    'weight_decay': 0.0,\n",
    "                    'cuda': False,\n",
    "                    'verbose': True,\n",
    "                    'train': True,\n",
    "                    'save_dir': './vae_model/',\n",
    "                },\n",
    "                \"tree_params\": {\n",
    "                    \"min_entries_per_label\": min_entries_per_label,\n",
    "                    \"grid_search_jobs\": -1,\n",
    "                    \"min_weight_gini\": 100, # set to 0.5 since here both class have same prob,\n",
    "                    \"max_search\" : 50,\n",
    "                    \"grid_search\": {\"cv\": 1,\"splitter\": [\"best\"],\"criterion\": [\"gini\"],\"max_depth\": [3,4,5,6,7,8,9,10],\n",
    "                                    \"min_samples_split\": [1.0,2,3],\"min_samples_leaf\": [1,2,3],\n",
    "                                    \"max_features\": [0.4, 0.6, 0.8],\n",
    "                                    }\n",
    "                }\n",
    "          }\n",
    "        print_conf(hpr)\n",
    "        return CEnt(data_models.trainData, mlmodel, hpr, data_catalog= data_models.new_catalog_n)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Recourse method not known  {}\".format(method))\n",
    "\n",
    "\n",
    "setup_catalog = load_setup()\n",
    "\n",
    "# data_names = ['adult', 'compas', 'give_me_some_credit', 'heloc']\n",
    "supported_backend_dict = {'pytorch': [\"cchvae\", \"clue\", \"cruds\", \"dice\", \"face\", 'growing_spheres',\"revise\" 'wachter', \n",
    "                                    'causal_recourse','actionable_recourse'],\n",
    "                        'tensorflow': ['cem', 'dice', 'face', 'growing_spheres', 'causal_recourse','actionable_recourse','cent'],\n",
    "                        'sklearn': ['feature_tweak','focus'],\n",
    "                        'xgboost': ['feature_tweak','focus']}\n",
    "\n",
    "\n",
    "# VAE distance in benchmarking                                                  DONE\n",
    "# Github migration                                                              DONE\n",
    "# clue, dice, face, growing_spheres, [focus, cem,] crude, wama tayasar          ~DONE\n",
    "# VAE latent representation layer size according to data columns                ~DONE\n",
    "# VAE constraint\n",
    "# VAE encodings distance in benchmarking                                        DONE\n",
    "# Implement our working version of VAE (tested on MNIST with 2 neurons)         DONE\n",
    "# Fix Best Metric in our version of VAE (load the best instead of using latest) DONE\n",
    "# Save Benchmark results per row, factuals, counterfactuals                     DONE\n",
    "# Save Tree results per row (Some time is added for inf)                        DONE\n",
    "#       For (3124, 15) nearest neighbors, we have the following timings: \n",
    "#               1. DTree fitting per row: 11.6 ms ± 1.72 ms per run\n",
    "#               2. Scoring (Inference+Score): 3.9 ms ± 126 µs per run\n",
    "\n",
    "\n",
    "\n",
    "FACTUAL_NUMBER = 20\n",
    "\n",
    "data_names = ['adult','compas', 'give_me_some_credit', 'heloc']\n",
    "\n",
    "recourse_methods = ['cent','dice','growing_spheres','clue','causal_recourse',\n",
    "                    'cchvae','cruds','focus','actionable_recourse',\n",
    "                    'cem','revisewachter','face','feature_tweak']\n",
    "\n",
    "NOTWORKING = [] # ['causal_recourse','focus'] # NOTWORKING\n",
    "TESTEDSUCCESSFULLY = ['clue','dice','cent','cchvae'] # ALREADY TESTED\n",
    "\n",
    "\n",
    "data_names = ['adult','heloc']\n",
    "\n",
    "recourse_methods = ['cent','dice','growing_spheres']\n",
    "\n",
    "\n",
    "# Define Output Directory\n",
    "OUT_DIR = \"./outputs/\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "print(recourse_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... --- logs will be saved to ./outputs/adult/models_logs.txt\n",
      "[WARNING] From /home/mmansour/Music/AUB/Julia/TreeBased_Carla/carla/models/catalog/ANN_TF/model_ann.py:10: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      " [deprecation_wrapper.py __getattr__]\n",
      "[WARNING] From /home/mmansour/miniconda3/envs/xai/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      " [deprecation_wrapper.py __getattr__]\n",
      "Starting VAE for benchmarking\n",
      "    myvae_params : \n",
      "         input_dim : 13\n",
      "         kld_weight : 0.00025\n",
      "         layers : [25]\n",
      "         latent_dim : 8\n",
      "         hidden_activation : relu\n",
      "         dropout : 0.2\n",
      "         batch_norm : True\n",
      "         batch_size : 32\n",
      "         epochs : 20\n",
      "         learning_rate : 0.001\n",
      "         weight_decay : 0.0\n",
      "         cuda : False\n",
      "         verbose : True\n",
      "         train : True\n",
      "         save_dir : ./vae_model/\n"
     ]
    }
   ],
   "source": [
    "data_name = data_names[0]\n",
    "OUT_DIR_DATA = os.path.join(OUT_DIR, data_name)\n",
    "if not os.path.exists(OUT_DIR_DATA):\n",
    "    os.makedirs(OUT_DIR_DATA)\n",
    "\n",
    "OUT_DIR_DATA_BENCH_CSVS = os.path.join(OUT_DIR_DATA, 'bench_csvs')\n",
    "if not os.path.exists(OUT_DIR_DATA_BENCH_CSVS):\n",
    "    os.makedirs(OUT_DIR_DATA_BENCH_CSVS)\n",
    "\n",
    "# Load dataset and necessary models\n",
    "data_models = DataModels(data_name = data_name,\n",
    "                            factuals_length = FACTUAL_NUMBER,\n",
    "                            out_dir = OUT_DIR_DATA)\n",
    "\n",
    "# Load VAE\n",
    "print(\"Starting VAE for benchmarking\")\n",
    "# Get an ann tensorflow model as temp just to get some hyperparams\n",
    "temp_model = data_models.models_zoo['ann']['tensorflow']\n",
    "\n",
    "if len(temp_model.feature_input_order) > 500:\n",
    "    layers = [500, 250]\n",
    "    latent_dim = 32\n",
    "elif len(temp_model.feature_input_order) > 100:\n",
    "    layers = [100, 50]\n",
    "    latent_dim = 24\n",
    "elif len(temp_model.feature_input_order) > 50:\n",
    "    layers = [50, 25]\n",
    "    latent_dim = 16\n",
    "elif len(temp_model.feature_input_order) > 20:\n",
    "    layers = [25, 16]\n",
    "    latent_dim = 12\n",
    "elif len(temp_model.feature_input_order) > 10:\n",
    "    layers = [25]\n",
    "    latent_dim = 8\n",
    "else:\n",
    "    layers = [16]\n",
    "    latent_dim = 7\n",
    "xxmutables = []\n",
    "for i in range(len(temp_model.feature_input_order)):\n",
    "    xxmutables.append(True)\n",
    "xxmutables = np.array(xxmutables)\n",
    "vae_parms = { \n",
    "    \"myvae_params\": {\n",
    "        'input_dim': len(temp_model.feature_input_order),\n",
    "        'kld_weight': 0.00025,\n",
    "        'layers': layers,\n",
    "        'latent_dim': latent_dim,\n",
    "        'hidden_activation': 'relu',\n",
    "        'dropout': 0.2,\n",
    "        'batch_norm': True,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 20,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 0.0,\n",
    "        'cuda': False,\n",
    "        'verbose': True,\n",
    "        'train': True,\n",
    "        'save_dir': './vae_model/',\n",
    "    }\n",
    "}\n",
    "print_conf(vae_parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scores = []\n",
    "# Define csv file to store results\n",
    "csv_file = os.path.join(OUT_DIR_DATA, 'benchmark_results.csv')\n",
    "recourse_method = 'cent'\n",
    "# Define Checkers\n",
    "test_checks = {'Resource_Method':[], 'Success_Boolean': [], 'model_type':[],'Details':[]}\n",
    "check_csv = os.path.join(OUT_DIR_DATA, 'checks.csv')\n",
    "# Loop over recourse methods\n",
    "supported_backend = get_resource_supported_backend(recourse_method, supported_backend_dict)\n",
    "if supported_backend in ['tensorflow', 'pytorch']:\n",
    "    supported_types = ['linear', 'ann']\n",
    "else:\n",
    "    supported_types = ['forest']\n",
    "supported_type = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temp = data_models.models_zoo[supported_type][supported_backend]\n",
    "if recourse_method in setup_catalog:\n",
    "    if 'hyperparams' in setup_catalog[recourse_method]:\n",
    "        hyperpars = setup_catalog[recourse_method]['hyperparams']\n",
    "else:\n",
    "    hyperpars = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# copy data_models.trainData\n",
    "copy_dataset = deepcopy(data_models.trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    data_name : data_name\n",
      "    n_search_samples : 300\n",
      "    p_norm : 1\n",
      "    step : 0.1\n",
      "    max_iter : 10\n",
      "    clamp : True\n",
      "    binary_cat_features : True\n",
      "    myvae_params : \n",
      "         input_dim : 13\n",
      "         kld_weight : 0.00025\n",
      "         layers : [25]\n",
      "         latent_dim : 8\n",
      "         hidden_activation : relu\n",
      "         dropout : 0.2\n",
      "         batch_norm : True\n",
      "         batch_size : 128\n",
      "         epochs : 1\n",
      "         learning_rate : 0.001\n",
      "         weight_decay : 0.0\n",
      "         cuda : False\n",
      "         verbose : True\n",
      "         train : True\n",
      "         save_dir : ./vae_model/\n",
      "    tree_params : \n",
      "         min_entries_per_label : 975\n",
      "         grid_search_jobs : -1\n",
      "         min_weight_gini : 100\n",
      "         max_search : 50\n",
      "         grid_search : \n",
      "              cv : 1\n",
      "              splitter : ['best']\n",
      "              criterion : ['gini']\n",
      "              max_depth : [3, 4, 5, 6, 7, 8, 9, 10]\n",
      "              min_samples_split : [1.0, 2, 3]\n",
      "              min_samples_leaf : [1, 2, 3]\n",
      "              max_features : [0.4, 0.6, 0.8]\n",
      "{'input_dim': 13, 'kld_weight': 0.00025, 'layers': [25], 'latent_dim': 8, 'hidden_activation': 'relu', 'dropout': 0.2, 'batch_norm': True, 'batch_size': 128, 'epochs': 1, 'learning_rate': 0.001, 'weight_decay': 0.0, 'cuda': False, 'verbose': True, 'train': True, 'save_dir': './vae_model/'}\n",
      "./vae_model/adult\n",
      "Epoch: 0, ELBO Loss: 262.2007141113281, Test MSELoss: 111.66492462158203\n",
      "Epoch: 0, Best ELBO Loss: 262.2007141113281\n",
      "DT Warming Up...\n",
      "{'criterion': 'gini', 'max_depth': 7, 'max_features': 0.4, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "rcmethod = intialialize_recourse_method(recourse_method, hyperpars, model_temp, data_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227671614860986"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get accuracy score\n",
    "# Import get_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_true = copy_dataset.df[model_temp.data.target].values\n",
    "y_pred = rcmethod.dataset[model_temp.data.target].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cem', 'cote', 'dice', 'growing_spheres']\n",
      "heloc\n"
     ]
    }
   ],
   "source": [
    "setup_catalog = load_setup()\n",
    "\n",
    "# data_names = ['adult', 'compas', 'give_me_some_credit', 'heloc']\n",
    "supported_backend_dict = {'pytorch': [\"cchvae\", \"clue\", \"cruds\", \"dice\", \"face\", 'growing_spheres',\"revise\" 'wachter', \n",
    "                                    'causal_recourse','actionable_recourse'],\n",
    "                        'tensorflow': ['cem', 'dice', 'face', 'growing_spheres', 'causal_recourse','actionable_recourse','cent'],\n",
    "                        'sklearn': ['feature_tweak','focus'],\n",
    "                        'xgboost': ['feature_tweak','focus','cent']}\n",
    "FACTUAL_NUMBER = 20\n",
    "\n",
    "data_names = ['adult','compas', 'give_me_some_credit', 'heloc']\n",
    "\n",
    "recourse_methods = ['cent','dice','growing_spheres','clue','causal_recourse',\n",
    "                    'cchvae','cruds','focus','actionable_recourse',\n",
    "                    'cem','revisewachter','face','feature_tweak']\n",
    "\n",
    "NOTWORKING = [] # ['causal_recourse','focus'] # NOTWORKING\n",
    "TESTEDSUCCESSFULLY = ['clue','dice','cent','cchvae'] # ALREADY TESTED\n",
    "\n",
    "\n",
    "data_names = ['adult','heloc']\n",
    "\n",
    "recourse_methods = ['cem','cent','dice','growing_spheres']\n",
    "\n",
    "\n",
    "# Define Output Directory\n",
    "OUT_DIR = \"./outputs/\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "print(recourse_methods)\n",
    "data_name = data_names[1]\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... --- logs will be saved to ./outputs/heloc/models_logs.txt\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR_DATA = os.path.join(OUT_DIR, data_name)\n",
    "if not os.path.exists(OUT_DIR_DATA):\n",
    "    os.makedirs(OUT_DIR_DATA)\n",
    "\n",
    "OUT_DIR_DATA_BENCH_CSVS = os.path.join(OUT_DIR_DATA, 'bench_csvs')\n",
    "if not os.path.exists(OUT_DIR_DATA_BENCH_CSVS):\n",
    "    os.makedirs(OUT_DIR_DATA_BENCH_CSVS)\n",
    "\n",
    "# Load dataset and necessary models\n",
    "data_models = DataModels(data_name = data_name,\n",
    "                            factuals_length = FACTUAL_NUMBER,\n",
    "                            out_dir = OUT_DIR_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = data_models.models_zoo['ann']['tensorflow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperpars = setup_catalog['cem']['hyperparams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] From /tmp/ipykernel_77298/3545653047.py:1: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      " [deprecation_wrapper.py __getattr__]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import Graph, Session\n",
    "ann_sess = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6909 samples, validate on 2962 samples\n",
      "Epoch 1/5\n",
      "6909/6909 [==============================] - 0s 41us/step - loss: 0.0945 - val_loss: 0.0419\n",
      "Epoch 2/5\n",
      "6909/6909 [==============================] - 0s 13us/step - loss: 0.0302 - val_loss: 0.0256\n",
      "Epoch 3/5\n",
      "6909/6909 [==============================] - 0s 14us/step - loss: 0.0245 - val_loss: 0.0218\n",
      "Epoch 4/5\n",
      "6909/6909 [==============================] - 0s 14us/step - loss: 0.0194 - val_loss: 0.0165\n",
      "Epoch 5/5\n",
      "6909/6909 [==============================] - 0s 12us/step - loss: 0.0157 - val_loss: 0.0142\n"
     ]
    }
   ],
   "source": [
    "def intialialize_recourse_method(method, hyperparams, mlmodel, data_models, ann_sess_ = None):\n",
    "    # TODO restrict data to training only\n",
    "    if method == \"cchvae\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        hyperparams[\"vae_params\"][\"layers\"] = [\n",
    "            len(mlmodel.feature_input_order)\n",
    "        ] + hyperparams[\"vae_params\"][\"layers\"]\n",
    "        return CCHVAE(mlmodel, hyperparams)\n",
    "    elif \"cem\" in method:\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        return CEM(ann_sess_, mlmodel, hyperparams)\n",
    "        #return CEM(sess, mlmodel, hyperparams)\n",
    "    elif method == \"clue\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        return Clue(mlmodel.data, mlmodel, hyperparams)\n",
    "    elif method == \"cruds\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        # variable input layer dimension is first time here available\n",
    "        hyperparams[\"vae_params\"][\"layers\"] = [\n",
    "            len(mlmodel.feature_input_order)\n",
    "        ] + hyperparams[\"vae_params\"][\"layers\"]\n",
    "        return CRUD(mlmodel, hyperparams)\n",
    "    elif method == \"dice\":\n",
    "        return Dice(mlmodel, hyperparams)\n",
    "    elif \"face\" in method:\n",
    "        return Face(mlmodel, hyperparams)\n",
    "    elif method == \"growing_spheres\":\n",
    "        return GrowingSpheres(mlmodel)\n",
    "    elif method == \"revise\":\n",
    "        hyperparams[\"data_name\"] = data_name\n",
    "        # variable input layer dimension is first time here available\n",
    "        hyperparams[\"vae_params\"][\"layers\"] = [\n",
    "            len(mlmodel.feature_input_order)\n",
    "        ] + hyperparams[\"vae_params\"][\"layers\"]\n",
    "        return Revise(mlmodel, mlmodel.data, hyperparams)\n",
    "    elif \"wachter\" in method:\n",
    "        return Wachter(mlmodel, hyperparams)\n",
    "    elif \"causal_recourse\" in method:\n",
    "        return CausalRecourse(mlmodel, hyperparams)\n",
    "    elif \"focus\" in method:\n",
    "        hyperparams = {'optimizer': 'adam', 'lr': 0.001, 'n_class': 2, 'n_iter': 1000, 'sigma': 1.0, 'temperature': 1.0, 'distance_weight': 0.01, 'distance_func': 'l1'}\n",
    "        return FOCUS(mlmodel, hyperparams)\n",
    "    elif \"feature_tweak\" in method:\n",
    "        return FOCUS(mlmodel, hyperparams)\n",
    "    elif \"cent\" in method:\n",
    "        min_entries_per_label = int(data_models.trainData.df.shape[0]*0.02)\n",
    "        if min_entries_per_label<900:\n",
    "            min_entries_per_label = 900\n",
    "        hpr = {\"data_name\": \"data_name\",\"n_search_samples\": 300,\"p_norm\": 1,\"step\": 0.1,\"max_iter\": 10,\"clamp\": True,\n",
    "                \"binary_cat_features\": True,\n",
    "                \"myvae_params\": {\n",
    "                    'input_dim': len(mlmodel.feature_input_order),\n",
    "                    'kld_weight': 0.00025,\n",
    "                    'layers': layers,\n",
    "                    'latent_dim': latent_dim,\n",
    "                    'hidden_activation': 'relu',\n",
    "                    'dropout': 0.2,\n",
    "                    'batch_norm': True,\n",
    "                    'batch_size': 32,\n",
    "                    'epochs': 10,\n",
    "                    'learning_rate': 0.001,\n",
    "                    'weight_decay': 0.0,\n",
    "                    'cuda': False,\n",
    "                    'verbose': True,\n",
    "                    'train': True,\n",
    "                    'save_dir': './vae_model/',\n",
    "                },\n",
    "                \"tree_params\": {\n",
    "                    \"min_entries_per_label\": min_entries_per_label,\n",
    "                    \"grid_search_jobs\": -1,\n",
    "                    \"min_weight_gini\": 100, # set to 0.5 since here both class have same prob,\n",
    "                    \"max_search\" : 100,\n",
    "                    \"grid_search\": {\"cv\": 1,\"splitter\": [\"best\"],\"criterion\": [\"gini\"],\"max_depth\": [3,4,5,6,7,8,9,10],\n",
    "                                    \"min_samples_split\": [1.0,2,3],\"min_samples_leaf\": [1,2,3],\n",
    "                                    \"max_features\": [0.4, 0.6, 0.8],\n",
    "                                    }\n",
    "                }\n",
    "          }\n",
    "        print_conf(hpr)\n",
    "        return CEnt(data_models.trainData, mlmodel, hpr, data_catalog= data_models.new_catalog_n)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Recourse method not known  {}\".format(method))\n",
    "\n",
    "\n",
    "rcmethod = intialialize_recourse_method('cem', hyperpars, mlmodel, data_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    data_name : data_name\n",
      "    n_search_samples : 300\n",
      "    p_norm : 1\n",
      "    step : 0.1\n",
      "    max_iter : 10\n",
      "    clamp : True\n",
      "    binary_cat_features : True\n",
      "    myvae_params : \n",
      "         input_dim : 21\n",
      "         kld_weight : 0.00025\n",
      "         layers : [25]\n",
      "         latent_dim : 6\n",
      "         hidden_activation : relu\n",
      "         dropout : 0.2\n",
      "         batch_norm : True\n",
      "         batch_size : 128\n",
      "         epochs : 1\n",
      "         learning_rate : 0.001\n",
      "         weight_decay : 0.0\n",
      "         cuda : False\n",
      "         verbose : True\n",
      "         train : True\n",
      "         save_dir : ./vae_model/\n",
      "    tree_params : \n",
      "         min_entries_per_label : 900\n",
      "         grid_search_jobs : -1\n",
      "         min_weight_gini : 100\n",
      "         max_search : 100\n",
      "         grid_search : \n",
      "              cv : 1\n",
      "              splitter : ['best']\n",
      "              criterion : ['gini']\n",
      "              max_depth : [3, 4, 5, 6, 7, 8, 9, 10]\n",
      "              min_samples_split : [1.0, 2, 3]\n",
      "              min_samples_leaf : [1, 2, 3]\n",
      "              max_features : [0.4, 0.6, 0.8]\n",
      "{'input_dim': 21, 'kld_weight': 0.00025, 'layers': [25], 'latent_dim': 6, 'hidden_activation': 'relu', 'dropout': 0.2, 'batch_norm': True, 'batch_size': 128, 'epochs': 1, 'learning_rate': 0.001, 'weight_decay': 0.0, 'cuda': False, 'verbose': True, 'train': True, 'save_dir': './vae_model/'}\n",
      "./vae_model/heloc\n",
      "Epoch: 0, ELBO Loss: 521.635009765625, Test MSELoss: 261.45343017578125\n",
      "Epoch: 0, Best ELBO Loss: 521.635009765625\n",
      "DT Warming Up...\n",
      "{'criterion': 'gini', 'max_depth': 5, 'max_features': 0.8, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "layers = [25]\n",
    "latent_dim = 6\n",
    "\n",
    "min_entries_per_label = int(data_models.trainData.df.shape[0]*0.02)\n",
    "if min_entries_per_label<900:\n",
    "    min_entries_per_label = 900\n",
    "hpr = {\"data_name\": \"data_name\",\"n_search_samples\": 300,\"p_norm\": 1,\"step\": 0.1,\"max_iter\": 10,\"clamp\": True,\n",
    "        \"binary_cat_features\": True,\n",
    "        \"myvae_params\": {\n",
    "            'input_dim': len(mlmodel.feature_input_order),\n",
    "            'kld_weight': 0.00025,\n",
    "            'layers': layers,\n",
    "            'latent_dim': latent_dim,\n",
    "            'hidden_activation': 'relu',\n",
    "            'dropout': 0.2,\n",
    "            'batch_norm': True,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 1,\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 0.0,\n",
    "            'cuda': False,\n",
    "            'verbose': True,\n",
    "            'train': True,\n",
    "            'save_dir': './vae_model/',\n",
    "        },\n",
    "        \"tree_params\": {\n",
    "            \"min_entries_per_label\": min_entries_per_label,\n",
    "            \"grid_search_jobs\": -1,\n",
    "            \"min_weight_gini\": 100, # set to 0.5 since here both class have same prob,\n",
    "            \"max_search\" : 100,\n",
    "            \"grid_search\": {\"cv\": 1,\"splitter\": [\"best\"],\"criterion\": [\"gini\"],\"max_depth\": [3,4,5,6,7,8,9,10],\n",
    "                            \"min_samples_split\": [1.0,2,3],\"min_samples_leaf\": [1,2,3],\n",
    "                            \"max_features\": [0.4, 0.6, 0.8],\n",
    "                            }\n",
    "        }\n",
    "    }\n",
    "print_conf(hpr)\n",
    "rcmethod = CEnt(data_models.trainData, mlmodel, hpr, data_catalog= data_models.new_catalog_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = data_models.models_zoo['ann']['tensorflow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6909 samples, validate on 2962 samples\n",
      "Epoch 1/5\n",
      "6909/6909 [==============================] - 0s 25us/step - loss: 0.0889 - val_loss: 0.0390\n",
      "Epoch 2/5\n",
      "6909/6909 [==============================] - 0s 12us/step - loss: 0.0288 - val_loss: 0.0224\n",
      "Epoch 3/5\n",
      "6909/6909 [==============================] - 0s 11us/step - loss: 0.0201 - val_loss: 0.0186\n",
      "Epoch 4/5\n",
      "6909/6909 [==============================] - 0s 12us/step - loss: 0.0177 - val_loss: 0.0167\n",
      "Epoch 5/5\n",
      "6909/6909 [==============================] - 0s 14us/step - loss: 0.0159 - val_loss: 0.0145\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"dense_1_1/kernel/Read/ReadVariableOp:0\", shape=(21, 13), dtype=float32) must be from the same graph as Tensor(\"Variable_1/read:0\", shape=(1, 21), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77298/3336945376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mann_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mann_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mrcmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintialialize_recourse_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperpars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcmethod\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdata_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactuals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_77298/2025550506.py\u001b[0m in \u001b[0;36mintialialize_recourse_method\u001b[0;34m(method, hyperparams, mlmodel, data_models)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m\"cem\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCEM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#return CEM(sess, mlmodel, hyperparams)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"clue\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Music/AUB/Julia/TreeBased_Carla/carla/recourse_methods/catalog/cem/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, mlmodel, hyperparams)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_dist_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_l_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ToEnforceLabel_Score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mToEnforceLabel_Score_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adv_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Music/AUB/Julia/TreeBased_Carla/carla/recourse_methods/catalog/cem/model.py\u001b[0m in \u001b[0;36m_get_label_score\u001b[0;34m(self, delta, adv)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0menforce_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PP\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mlmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menforce_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_l_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    749\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mixed_precision_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_cast_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5923\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5924\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5925\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   5926\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5927\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    364\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   6133\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6134\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6135\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6136\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6137\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   6069\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6070\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[0;32m-> 6071\u001b[0;31m                      (item, original_item))\n\u001b[0m\u001b[1;32m   6072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"dense_1_1/kernel/Read/ReadVariableOp:0\", shape=(21, 13), dtype=float32) must be from the same graph as Tensor(\"Variable_1/read:0\", shape=(1, 21), dtype=float32)."
     ]
    }
   ],
   "source": [
    "graph = Graph()\n",
    "with graph.as_default():\n",
    "    ann_sess = Session()\n",
    "    with ann_sess.as_default():\n",
    "        rcmethod = intialialize_recourse_method('cem', hyperpars, mlmodel, data_models, ann_sess_ = ann_sess)\n",
    "        benchmark = Benchmark(mlmodel, rcmethod,  data_models.factuals.copy().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = [\n",
    "                    evaluation_catalog.YNN(benchmark.mlmodel, {\"y\": 5, \"cf_label\": 1}),\n",
    "                    evaluation_catalog.Distance(benchmark.mlmodel),\n",
    "                    evaluation_catalog.SuccessRate(),\n",
    "                    evaluation_catalog.Redundancy(benchmark.mlmodel, {\"cf_label\": 1}),\n",
    "                    evaluation_catalog.ConstraintViolation(benchmark.mlmodel),\n",
    "                    evaluation_catalog.AvgTime({\"time\": benchmark.timer})                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_bench = benchmark.run_benchmark(measures = measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y-Nearest-Neighbours    0.457143\n",
       "L0_distance             2.571429\n",
       "L1_distance             0.253065\n",
       "L2_distance             0.050363\n",
       "Linf_distance           0.151970\n",
       "Success_Rate            0.700000\n",
       "Redundancy              1.357143\n",
       "Constraint_Violation    0.000000\n",
       "avg_time                0.181596\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource_bench.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7d092e1e62380f8fbf298e57116b8df576168f9d2d30197b9cdcbc5ff4cbd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
