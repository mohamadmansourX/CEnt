{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compas_June212022 Carla_Wrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamadmansourX/TreeBased_Carla/blob/main/compas_June212022_Carla_Wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeafNode ---> merge_conditions\n",
        "\n",
        "if both has '<=' we sould choose the min value but its not working unless we choose max??? same to '>'"
      ],
      "metadata": {
        "id": "XDzs2eomsMFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carla-recourse/CARLA.git\n",
        "%cd CARLA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCeHsIfWeGFH",
        "outputId": "4d022f99-17f0-4de4-956d-d6352184766d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CARLA'...\n",
            "remote: Enumerating objects: 2210, done.\u001b[K\n",
            "remote: Counting objects: 100% (1012/1012), done.\u001b[K\n",
            "remote: Compressing objects: 100% (626/626), done.\u001b[K\n",
            "remote: Total 2210 (delta 643), reused 481 (delta 386), pack-reused 1198\u001b[K\n",
            "Receiving objects: 100% (2210/2210), 2.10 MiB | 9.33 MiB/s, done.\n",
            "Resolving deltas: 100% (1239/1239), done.\n",
            "/content/CARLA/CARLA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset --hard 87983228739b616f46481960a1da77e13b05c974\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5o6QIGDtkeG",
        "outputId": "bc7fdf8e-03d9-45e4-f29a-90747825f66e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEAD is now at 8798322 Chore/fix ordering bug (#150)\n",
            "Updating 8798322..648b2aa\n",
            "Fast-forward\n",
            " README.md                                          |   4 \u001b[32m+\u001b[m\n",
            " carla/evaluation/distances.py                      |   6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/models/catalog/ANN_TORCH/model_ann.py        |  54 \u001b[31m----\u001b[m\n",
            " carla/models/catalog/Linear_TORCH/model_linear.py  |  40 \u001b[31m---\u001b[m\n",
            " carla/plotting/__init__.py                         |   3 \u001b[32m+\u001b[m\n",
            " carla/plotting/barplot.py                          |  27 \u001b[32m++\u001b[m\n",
            " carla/plotting/plotting.py                         | 102 \u001b[32m++++++++\u001b[m\n",
            " carla/plotting/stripplot.py                        |  62 \u001b[32m+++++\u001b[m\n",
            " carla/plotting/swarmplot.py                        |  51 \u001b[32m++++\u001b[m\n",
            " .../catalog/actionable_recourse/model.py           |   3 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/cchvae/model.py     |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/cem/model.py        |   3 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/clue/model.py       |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/crud/model.py       |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/dice/model.py       |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/face/model.py       |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " .../catalog/feature_tweak/model.py                 |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/focus/model.py      |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " .../catalog/growing_spheres/model.py               |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/revise/model.py     |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " .../catalog/wachter/library/wachter.py             |  41 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " carla/recourse_methods/catalog/wachter/model.py    |   5 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " .../recourse_methods/processing/counterfactuals.py |   7 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " docs/source/index.rst                              |   1 \u001b[32m+\u001b[m\n",
            " docs/source/notebooks/plotting_example.ipynb       | 290 \u001b[32m+++++++++++++++++++++\u001b[m\n",
            " docs/source/plotting.rst                           |  12 \u001b[32m+\u001b[m\n",
            " docs/source/tutorial.rst                           |   1 \u001b[32m+\u001b[m\n",
            " setup.py                                           |   1 \u001b[32m+\u001b[m\n",
            " test/test_cfmodel.py                               |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 29 files changed, 618 insertions(+), 119 deletions(-)\n",
            " create mode 100644 carla/plotting/__init__.py\n",
            " create mode 100644 carla/plotting/barplot.py\n",
            " create mode 100644 carla/plotting/plotting.py\n",
            " create mode 100644 carla/plotting/stripplot.py\n",
            " create mode 100644 carla/plotting/swarmplot.py\n",
            " create mode 100644 docs/source/notebooks/plotting_example.ipynb\n",
            " create mode 100644 docs/source/plotting.rst\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout 87983228739b616f46481960a1da77e13b05c974"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn-VNbJTwvtx",
        "outputId": "fb0a4da4-89f3-460a-9f57-83b08feef7f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: checking out '87983228739b616f46481960a1da77e13b05c974'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 8798322 Chore/fix ordering bug (#150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo \"\"\"pip install -U pip setuptools wheel\n",
        "pip install -e .\n",
        "pip install -r requirements-dev.txt\n",
        "pre-commit install\n",
        "pip install -r requirements-dev.txt\n",
        "\"\"\" > mm_bash_setup.sh"
      ],
      "metadata": {
        "id": "B6XGsT61XmOx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash mm_bash_setup.sh"
      ],
      "metadata": {
        "id": "KRyWxBcgX_wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f6d417-aff9-4b46-ef7a-a69bd70c3d9a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (62.6.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/CARLA/CARLA\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lime==0.2.0.1 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (0.2.0.1)\n",
            "Requirement already satisfied: mip==1.12.0 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (1.12.0)\n",
            "Requirement already satisfied: numpy==1.19.4 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (1.19.4)\n",
            "Collecting pandas==1.1.4\n",
            "  Using cached pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "Requirement already satisfied: recourse==1.0.0 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (0.23.2)\n",
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (1.14.0)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (1.7.0)\n",
            "Requirement already satisfied: torchvision==0.8.1 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (0.8.1)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (2.10.0)\n",
            "Requirement already satisfied: dice-ml==0.5 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (0.5)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (7.22.0)\n",
            "Requirement already satisfied: keras==2.3.0 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (2.3.0)\n",
            "Requirement already satisfied: xgboost==1.4.2 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (1.4.2)\n",
            "Requirement already satisfied: causalgraphicalmodels==0.0.4 in /usr/local/lib/python3.7/dist-packages (from carla-recourse==0.0.4) (0.0.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from causalgraphicalmodels==0.0.4->carla-recourse==0.0.4) (2.5.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from causalgraphicalmodels==0.0.4->carla-recourse==0.0.4) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->carla-recourse==0.0.4) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.0->carla-recourse==0.0.4) (6.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.0->carla-recourse==0.0.4) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.0->carla-recourse==0.0.4) (1.6.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.0->carla-recourse==0.0.4) (1.1.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->carla-recourse==0.0.4) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->carla-recourse==0.0.4) (4.64.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->carla-recourse==0.0.4) (3.2.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from mip==1.12.0->carla-recourse==0.0.4) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.4->carla-recourse==0.0.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.4->carla-recourse==0.0.4) (2022.1)\n",
            "Requirement already satisfied: prettytable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from recourse==1.0.0->carla-recourse==0.0.4) (3.3.0)\n",
            "Requirement already satisfied: cplex>=12.8 in /usr/local/lib/python3.7/dist-packages (from recourse==1.0.0->carla-recourse==0.0.4) (22.1.0.0)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from recourse==1.0.0->carla-recourse==0.0.4) (0.11.2)\n",
            "Requirement already satisfied: pytest>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from recourse==1.0.0->carla-recourse==0.0.4) (6.1.2)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from recourse==1.0.0->carla-recourse==0.0.4) (2.11.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from recourse==1.0.0->carla-recourse==0.0.4) (5.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->carla-recourse==0.0.4) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->carla-recourse==0.0.4) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (1.46.3)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->carla-recourse==0.0.4) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->carla-recourse==0.0.4) (4.1.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->carla-recourse==0.0.4) (0.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->carla-recourse==0.0.4) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1->carla-recourse==0.0.4) (7.1.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (62.6.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (3.0.29)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->carla-recourse==0.0.4) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython->carla-recourse==0.0.4) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->recourse==1.0.0->carla-recourse==0.0.4) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime==0.2.0.1->carla-recourse==0.0.4) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime==0.2.0.1->carla-recourse==0.0.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime==0.2.0.1->carla-recourse==0.0.4) (1.4.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython->carla-recourse==0.0.4) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable>=0.7.2->recourse==1.0.0->carla-recourse==0.0.4) (4.11.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable>=0.7.2->recourse==1.0.0->carla-recourse==0.0.4) (0.2.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest>=4.3.0->recourse==1.0.0->carla-recourse==0.0.4) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest>=4.3.0->recourse==1.0.0->carla-recourse==0.0.4) (21.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=4.3.0->recourse==1.0.0->carla-recourse==0.0.4) (1.1.1)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=4.3.0->recourse==1.0.0->carla-recourse==0.0.4) (0.13.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=4.3.0->recourse==1.0.0->carla-recourse==0.0.4) (21.4.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=4.3.0->recourse==1.0.0->carla-recourse==0.0.4) (1.11.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->carla-recourse==0.0.4) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->carla-recourse==0.0.4) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->carla-recourse==0.0.4) (2.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->carla-recourse==0.0.4) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->carla-recourse==0.0.4) (3.3.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->mip==1.12.0->carla-recourse==0.0.4) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable>=0.7.2->recourse==1.0.0->carla-recourse==0.0.4) (3.8.0)\n",
            "Installing collected packages: pandas, carla-recourse\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Running setup.py develop for carla-recourse\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.4 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.22.0 which is incompatible.\n",
            "fastai 2.6.3 requires torchvision>=0.8.2, but you have torchvision 0.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed carla-recourse pandas-1.1.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pre-commit==2.9.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 1)) (2.9.2)\n",
            "Requirement already satisfied: pytest==6.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 2)) (6.1.2)\n",
            "Requirement already satisfied: sphinx==4.0.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme==0.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 4)) (0.5.2)\n",
            "Requirement already satisfied: sphinx_autodoc_typehints==1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 5)) (1.12.0)\n",
            "Requirement already satisfied: numpydoc==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 7)) (2.9.0)\n",
            "Requirement already satisfied: ipython==7.22.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 8)) (7.22.0)\n",
            "Requirement already satisfied: jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 9)) (2.11.3)\n",
            "Requirement already satisfied: networkx==2.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 10)) (2.5.1)\n",
            "Requirement already satisfied: scipy==1.6.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 11)) (1.6.2)\n",
            "Requirement already satisfied: markupsafe==2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 12)) (2.0.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: virtualenv>=20.0.8 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (20.14.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (3.3.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (4.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (2.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (0.13.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (21.4.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (1.11.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.10.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (0.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (62.6.0)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.9.0->-r requirements-dev.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.9.0->-r requirements-dev.txt (line 7)) (1.19.4)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (3.0.29)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (5.1.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2022.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (2.5.2)\n",
            "Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytest==6.1.2->-r requirements-dev.txt (line 2)) (3.0.9)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mpre-commit installed at .git/hooks/pre-commit\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pre-commit==2.9.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 1)) (2.9.2)\n",
            "Requirement already satisfied: pytest==6.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 2)) (6.1.2)\n",
            "Requirement already satisfied: sphinx==4.0.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme==0.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 4)) (0.5.2)\n",
            "Requirement already satisfied: sphinx_autodoc_typehints==1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 5)) (1.12.0)\n",
            "Requirement already satisfied: numpydoc==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 7)) (2.9.0)\n",
            "Requirement already satisfied: ipython==7.22.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 8)) (7.22.0)\n",
            "Requirement already satisfied: jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 9)) (2.11.3)\n",
            "Requirement already satisfied: networkx==2.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 10)) (2.5.1)\n",
            "Requirement already satisfied: scipy==1.6.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 11)) (1.6.2)\n",
            "Requirement already satisfied: markupsafe==2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 12)) (2.0.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: virtualenv>=20.0.8 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (20.14.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (3.3.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (4.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (2.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (0.13.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (21.4.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest==6.1.2->-r requirements-dev.txt (line 2)) (1.11.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.10.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (0.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (62.6.0)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.7/dist-packages (from sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.9.0->-r requirements-dev.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.9.0->-r requirements-dev.txt (line 7)) (1.19.4)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (3.0.29)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.22.0->-r requirements-dev.txt (line 8)) (5.1.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2022.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.22.0->-r requirements-dev.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx==4.0.2->-r requirements-dev.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (2.5.2)\n",
            "Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit==2.9.2->-r requirements-dev.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytest==6.1.2->-r requirements-dev.txt (line 2)) (3.0.9)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pandas -y\n",
        "!pip install pandas==1.3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhnmWheUdCOu",
        "outputId": "630e47ec-4bb1-40a1-d2fb-780a5d16aa94"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 1.1.4\n",
            "Uninstalling pandas-1.1.4:\n",
            "  Successfully uninstalled pandas-1.1.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.3.5\n",
            "  Using cached pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (1.19.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.4 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.22.0 which is incompatible.\n",
            "fastai 2.6.3 requires torchvision>=0.8.2, but you have torchvision 0.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the Model"
      ],
      "metadata": {
        "id": "FeLj5teOseo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CARLA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBfbSJQhYfZJ",
        "outputId": "c4f314b6-e9e0-4947-e94d-5c75977c3d6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CARLA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import xgboost\n",
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "def seed_my_session(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  tf.set_random_seed(42)\n",
        "  sklearn.random_state = seed\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "seed_my_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP9hUXh4-MgY",
        "outputId": "d072e9af-c029-4b46-9ef4-f8438ade794f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple, Union\n",
        "import pandas as pd\n",
        "from carla import RecourseMethod\n",
        "from carla.data.api import data, Data\n",
        "from carla.models.api import MLModel\n",
        "from carla.recourse_methods.autoencoder import (\n",
        "    VAEDataset,\n",
        "    VariationalAutoencoder,\n",
        "    train_variational_autoencoder,\n",
        ")\n",
        "from carla.recourse_methods.processing import (\n",
        "    check_counterfactuals,\n",
        "    merge_default_parameters,\n",
        "    reconstruct_encoding_constraints,\n",
        ")\n",
        "# For Descision Tree implementation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import numpy as np\n",
        "from carla import Benchmark\n",
        "from carla.recourse_methods import Dice, Face\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "fXLi06EAPbyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4a39f2-bd54-45b3-9db6-78db7ec7b575"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "def load_setup() -> Dict:\n",
        "    with open(\"experimental_setup.yaml\", \"r\") as f:\n",
        "        setup_catalog = yaml.safe_load(f)\n",
        "    return setup_catalog[\"recourse_methods\"]\n",
        "setup = load_setup()"
      ],
      "metadata": {
        "id": "JR6mT9Y8Hi7R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tree Leaf utils\n",
        "\"\"\"\n",
        "\n",
        "# !git clone https://github.com/carla-recourse/CARLA.git\n",
        "\n",
        "\n",
        "import enum\n",
        "from typing import Dict, List, Tuple, Union\n",
        "import pandas as pd\n",
        "from carla import RecourseMethod\n",
        "from carla.data.api import data, Data\n",
        "from carla.models.api import MLModel\n",
        "from carla.recourse_methods.autoencoder import (\n",
        "    VAEDataset,\n",
        "    VariationalAutoencoder,\n",
        "    train_variational_autoencoder,\n",
        ")\n",
        "from carla.recourse_methods.processing import (\n",
        "    check_counterfactuals,\n",
        "    merge_default_parameters,\n",
        "    reconstruct_encoding_constraints,\n",
        ")\n",
        "# For Descision Tree implementation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import numpy as np\n",
        "from carla import Benchmark\n",
        "from carla.recourse_methods import Dice, Face\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "class LeafNode:\n",
        "    def __init__(self, conditions, label, weight):\n",
        "        # Conditions is a list of tuples from the root node to the leaf node\n",
        "        self.conditions = deepcopy(conditions)\n",
        "        # Label is the label of the leaf node\n",
        "        self.label = label\n",
        "        # Wieght Either entropy or gini\n",
        "        self.weight = weight\n",
        "        # Duplicate conditions\n",
        "        self.duplicate_conditions = []\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Print the leaf node with conditions and label\n",
        "        \"\"\"\n",
        "        return \"LeafNode(label={}, weight={}, conditions={})\".format(self.label, self.weight, self.conditions)\n",
        "\n",
        "    def compare_node(self, other): #TODO misleading name\n",
        "        \"\"\"\n",
        "        Get the distance between two leaf nodes by returning a set of conditions as follows:\n",
        "        1. Initialize conditions as other conditions\n",
        "        2. Remove conditions that are exactly the same with self\n",
        "        3. Return the remaining conditions\n",
        "\n",
        "        # TODO common feature\n",
        "        \"\"\"\n",
        "        # Initialize conditions as other conditions\n",
        "        conditions = other.conditions\n",
        "        # Remove conditions that are common with self\n",
        "        for condition in self.conditions:\n",
        "            conditions = [c for c in conditions if c != condition]\n",
        "        # Return the remaining conditions\n",
        "        return conditions\n",
        "\n",
        "    def merge_conditions(self):\n",
        "        \"\"\"\n",
        "        If there are two conditions with the same feature and threshold_sign, merge them into one condition\n",
        "        \"\"\"\n",
        "        # Initialize conditions as other conditions\n",
        "        # \n",
        "        conditions = self.conditions\n",
        "        # indexes to be dropped\n",
        "        indexes = []\n",
        "        # Search for conditions with the same feature and threshold_sign\n",
        "        for i in range(len(conditions)):\n",
        "            # if i in the indexes to be dropped, skip\n",
        "            if i in indexes:\n",
        "                continue\n",
        "            for j in range(i + 1, len(conditions)):\n",
        "                # if j in the indexes to be dropped, skip\n",
        "                if j in indexes:\n",
        "                    continue\n",
        "                if conditions[i].feature == conditions[j].feature:\n",
        "                    if conditions[i].threshold_sign == conditions[j].threshold_sign:\n",
        "                        # Merge the two conditions\n",
        "                        if conditions[i].threshold_sign == '<=':\n",
        "                            conditions[i].threshold = min(conditions[i].threshold, conditions[j].threshold)\n",
        "                        else:\n",
        "                            conditions[i].threshold = max(conditions[i].threshold, conditions[j].threshold)\n",
        "                        # Add index to drop\n",
        "                        indexes.append(j)\n",
        "                    else:\n",
        "                        # Add it to duplicate conditions\n",
        "                        if conditions[j].feature not in self.duplicate_conditions:\n",
        "                            self.duplicate_conditions.append(conditions[j].feature)\n",
        "        # Drop indexes from conditions\n",
        "        conditions = [c for i, c in enumerate(conditions) if i not in indexes]\n",
        "        self.conditions = conditions\n",
        "        self.duplicate_conditions = list(set(self.duplicate_conditions))\n",
        "\n",
        "    def check_point(self, point):\n",
        "        \"\"\"\n",
        "        Check if the point satisfies the conditions of the leaf node\n",
        "        \"\"\"\n",
        "        # Check if the point satisfies the conditions of the leaf node\n",
        "        for condition in self.conditions:\n",
        "            if not condition.check_point(point):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def generate_point(self, point, data_catalog = None, sigma =0.5, gamma = 0):\n",
        "        \"\"\"\n",
        "        Generate a point from a point\n",
        "        \"\"\"\n",
        "        # loop through the duplicate conditions\n",
        "        for feature in self.duplicate_conditions:\n",
        "            # get the two conditions with that feature\n",
        "            conditions = [c for c in self.conditions if c.feature == feature]\n",
        "            # data_catalog contains {'categorical':[],'continuous':[],'imutable':[], 'continuous_stats':[]}\n",
        "            if feature in data_catalog['categorical']:\n",
        "                # Assert that there shouldn't be duplicate conditions for a binary feature (categorical here are binaries)\n",
        "                assert False, \"There shouldn't be duplicate conditions for a binary feature\"\n",
        "                #TODO (general user he can't solve it)\n",
        "                # Thrsh can be continous, then generate a random point between threshold and round the result\n",
        "            elif feature in data_catalog['continuous']:\n",
        "                # using the continuous_stats get the std and mean\n",
        "                std = data_catalog['continuous_stats'][feature]['std']\n",
        "                mean = data_catalog['continuous_stats'][feature]['mean']\n",
        "                minn = data_catalog['continuous_stats'][feature]['min']\n",
        "                maxx = data_catalog['continuous_stats'][feature]['max']\n",
        "                # Using the mean, std, min and max create a bias value\n",
        "                # bias values is std/10 * (max - min)\n",
        "                bias = std / sigma # TOCHECK LATER\n",
        "                bias = min(bias, abs(conditions[0].threshold - conditions[1].threshold))\n",
        "                # Min\n",
        "                if gamma == 0:\n",
        "                    min_bias = 0\n",
        "                else:\n",
        "                    min_bias = std / gamma\n",
        "                # Generate a random value between the two thresholds\n",
        "                bias = random.uniform(min_bias, bias)\n",
        "                # Add the bias to the threshold\n",
        "                if conditions[0].threshold_sign == '<=':\n",
        "                    point[feature] = conditions[0].threshold + bias\n",
        "                else:\n",
        "                    point[feature] = conditions[1].threshold + bias\n",
        "        for condition in self.conditions:\n",
        "            if condition.feature not in self.duplicate_conditions:\n",
        "                if condition.feature in data_catalog['categorical']:\n",
        "                    # Simply flip the value\n",
        "                    # Round the threshold\n",
        "                    point[condition.feature] = not point[condition.feature]\n",
        "                else: # condition.feature in data_catalog['continuous']:\n",
        "                    std = data_catalog['continuous_stats'][condition.feature]['std']\n",
        "                    mean = data_catalog['continuous_stats'][condition.feature]['mean']\n",
        "                    minn = data_catalog['continuous_stats'][condition.feature]['min']\n",
        "                    maxx = data_catalog['continuous_stats'][condition.feature]['max']\n",
        "                    bias = std / sigma\n",
        "                    # Min\n",
        "                    if gamma == 0:\n",
        "                        min_bias = 0\n",
        "                    else:\n",
        "                        min_bias = std / gamma\n",
        "                    # Generate a random value between the two thresholds\n",
        "                    bias = random.uniform(min_bias, bias)\n",
        "                    if condition.threshold_sign == '<=':\n",
        "                        point[condition.feature] = condition.threshold + bias\n",
        "                    else:\n",
        "                        point[condition.feature] = condition.threshold - bias\n",
        "        return point\n",
        "\n",
        "\n",
        "class Condition:\n",
        "    def __init__(self, feature, threshold, threshold_sign):\n",
        "        # Feature is the feature name\n",
        "        self.feature = feature\n",
        "        # Value is the value of the feature\n",
        "        self.threshold = threshold\n",
        "        # <= or > since they are the only two threshold_sign in Decision Tree\n",
        "        self.threshold_sign = threshold_sign\n",
        "    def __repr__(self):\n",
        "        return f'{self.feature} {self.threshold_sign} {self.threshold}'\n",
        "    def check_point(self, point):\n",
        "        \"\"\"\n",
        "        Check if the point satisfies the condition\n",
        "        \"\"\"\n",
        "        # Check if the point satisfies the condition\n",
        "        if self.threshold_sign == '<=':\n",
        "            return point[self.feature] <= self.threshold\n",
        "        else:\n",
        "            return point[self.feature] > self.threshold\n",
        "\n",
        "\n",
        "class TreeLeafs:\n",
        "    def __init__(self, tree, feature_input_order):\n",
        "        self.tree = tree\n",
        "        self.feature_input_order = feature_input_order\n",
        "        self.leafs_nodes = []\n",
        "        self.get_leaf_nodes(tree)\n",
        "        for leaf in self.leafs_nodes:\n",
        "            leaf.merge_conditions()\n",
        "\n",
        "    def get_leaf_nodes(self, tree, node_id=0, conditions=[]):\n",
        "        \"\"\"\n",
        "        This will be a recursion function that will append to leaf_nodes list, their labels and set of conditions\n",
        "        If the node is a leaf node, then it will append a LeafNode object to leaf_nodes\n",
        "        If the node is not a leaf node, then it will return while adding the conditions of the left and right child to the list\n",
        "        \"\"\"\n",
        "        # If the node is a leaf node\n",
        "        if tree.children_left[node_id] == -1 and tree.children_right[node_id] == -1:\n",
        "            # Append the leaf node to the list\n",
        "            self.leafs_nodes.append(LeafNode(conditions, np.argmax(tree.value[node_id]), tree.impurity[node_id]))\n",
        "        # If the node is not a leaf node\n",
        "        else:\n",
        "            # Need to get the feature of the node\n",
        "            feature = self.feature_input_order[tree.feature[node_id]]\n",
        "            # Need to get the threshold of the node\n",
        "            threshold = tree.threshold[node_id]\n",
        "            # For right child if exists, threshold_sign is >\n",
        "            if tree.children_right[node_id] != -1:\n",
        "                conditions_right = conditions.copy()\n",
        "                # Append the condition to the list\n",
        "                conditions_right.append(Condition(feature, threshold, '>'))\n",
        "                # Get the right child\n",
        "                self.get_leaf_nodes(tree, tree.children_right[node_id], conditions_right)\n",
        "            # For left child if exists, threshold_sign is <=\n",
        "            if tree.children_left[node_id] != -1:\n",
        "                conditions_left = conditions.copy()\n",
        "                # Append the condition to the list\n",
        "                conditions_left.append(Condition(feature, threshold, '<='))\n",
        "                # Get the left child\n",
        "                self.get_leaf_nodes(tree, tree.children_left[node_id], conditions_left)\n"
      ],
      "metadata": {
        "id": "3pz8zIRCkF9U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "class TreeBasedContrastiveExplanation(RecourseMethod):\n",
        "    '''\n",
        "    Decision Tree Based contrastive explanations\n",
        "    '''\n",
        "    _DEFAULT_HYPERPARAMS = {\n",
        "      \"data_name\": None,\n",
        "      \"n_search_samples\": 300,\n",
        "      \"p_norm\": 1,\n",
        "      \"step\": 0.1,\n",
        "      \"max_iter\": 1000,\n",
        "      \"clamp\": True,\n",
        "      \"target_class\": [0, 1],\n",
        "      \"binary_cat_features\": True,\n",
        "      \"vae_params\": {\n",
        "          \"layers\": None,\n",
        "          \"train\": True,\n",
        "          \"lambda_reg\": 1e-6,\n",
        "          \"epochs\": 5,\n",
        "          \"lr\": 1e-3,\n",
        "          \"batch_size\": 32,\n",
        "      },\n",
        "      \"tree_params\": {\n",
        "          \"min_entries_per_label\": 1000,\n",
        "          \"grid_search_jobs\": -1,\n",
        "          \"min_weight_gini\": 100, # set to 0.5 since here both class have same prob\n",
        "          \"max_search\": 500,\n",
        "          \"grid_search\": {\n",
        "                \"splitter\": [\"best\"],\n",
        "                \"criterion\": [\"gini\"],\n",
        "                \"max_depth\": [6],\n",
        "                \"min_samples_split\": [2],\n",
        "                \"min_samples_leaf\": [1],\n",
        "                \"max_features\": [None] #Note changing this will result in removing features that we might want to keep\n",
        "          }\n",
        "      }\n",
        "\n",
        "    }\n",
        "\n",
        "    def __init__(self, dataset:Data, mlmodel: MLModel, hyperparams: Dict, data_catalog: Dict, distance_metric ='euclidean'):\n",
        "        super().__init__(mlmodel)\n",
        "        self.distance_metric = distance_metric\n",
        "        # Construct catalog\n",
        "        self.data_catalog = data_catalog\n",
        "        # Construct mlmodel\n",
        "        self.mlmodel = mlmodel\n",
        "        # Construct the hyperparameters\n",
        "        self.hyperparams = merge_default_parameters(hyperparams, self._DEFAULT_HYPERPARAMS)\n",
        "        # Construct the VAE\n",
        "        # self.vae = TEMP_VAE\n",
        "        self.vae = self.load_vae(dataset, self.hyperparams[\"vae_params\"], mlmodel, mlmodel.data.name)\n",
        "        # Construct the dataframe with encodings\n",
        "        self.dataset = dataset.df\n",
        "        self.dataset['VAE_ENCODED'] = self.get_encodeings(self.dataset.copy())\n",
        "        # Load Grid Parameters\n",
        "        self.hyperparams[\"tree_params\"][\"grid_search\"] = self.optimize_grid(self.hyperparams[\"tree_params\"][\"grid_search\"], self.dataset)\n",
        "        ## These are added to optimize neighbor sampling for DT which used to take ~0.4 seconds and now will be \n",
        "        # NNDescent\n",
        "        self.data_indexes_m = self.dataset.index\n",
        "        self.set_distance_metric_initialize_nn(self.distance_metric)\n",
        "    def set_distance_metric_initialize_nn(self, distance_metric):\n",
        "        self.distance_metric = distance_metric\n",
        "        self.nnd = NNDescent(np.array(self.dataset[\"VAE_ENCODED\"].values.tolist()), metric=self.distance_metric,random_state=42)\n",
        "        self.nnd.prepare()\n",
        "    def set_model(self,mlmodel):\n",
        "        self._mlmodel = mlmodel\n",
        "        self.mlmodel = mlmodel\n",
        "    def optimize_grid(self, grid_search, df):\n",
        "        \"\"\"\n",
        "        Optimize the grid search parameters\n",
        "        #@TODO: make it on chunkc of the dataframe and return the top\n",
        "        #@TODO: the chunkcs in order of encodings\n",
        "        \"\"\"\n",
        "        # Define Decision Tree Classifier\n",
        "        dec_tree = DecisionTreeClassifier(random_state=0)\n",
        "        # Define Grid Search\n",
        "        grid_search = GridSearchCV(dec_tree, grid_search, cv=5, n_jobs=self.hyperparams[\"tree_params\"][\"grid_search_jobs\"])\n",
        "        target_values = df[self._mlmodel.data.target]\n",
        "        train_features = df[self._mlmodel.feature_input_order]\n",
        "        # Fit the Grid Search\n",
        "        grid_search.fit(train_features, target_values)\n",
        "        # Return the best parameters as the format of grid_search\n",
        "        print(grid_search.best_params_)\n",
        "        return grid_search.best_params_\n",
        "\n",
        "    def load_vae(self, data: pd.DataFrame, vae_params: Dict, mlmodel: MLModel, data_name: str) -> VariationalAutoencoder:\n",
        "        '''\n",
        "        Load and train the VAE if needed\n",
        "        '''\n",
        "        generative_model = VariationalAutoencoder(data_name, vae_params['layers'])\n",
        "        # if train is True, train the VAE\n",
        "        if vae_params['train']:\n",
        "            generative_model = train_variational_autoencoder(\n",
        "                generative_model,\n",
        "                data,\n",
        "                mlmodel.feature_input_order,\n",
        "                lambda_reg=vae_params[\"lambda_reg\"],\n",
        "                epochs=vae_params[\"epochs\"],\n",
        "                lr=vae_params[\"lr\"],\n",
        "                batch_size=vae_params[\"batch_size\"],\n",
        "            )\n",
        "        else:\n",
        "            try:\n",
        "                # CHeck if the generative_model can load our data\n",
        "                generative_model.load(data.shape[1] - 1)\n",
        "            except FileNotFoundError as exc:\n",
        "                raise FileNotFoundError(\n",
        "                    \"Loading of Autoencoder failed. {}\".format(str(exc))\n",
        "                )\n",
        "        \n",
        "        return generative_model\n",
        "    \n",
        "    def get_counterfactuals(self, factuals: pd.DataFrame):\n",
        "        '''\n",
        "        this property is responsible to generate and output\n",
        "        encoded and scaled counterfactual examples\n",
        "        as pandas DataFrames\n",
        "        '''\n",
        "        # Get the encoded features of factuals\n",
        "        factuals[\"VAE_ENCODED\"] = self.get_encodeings(factuals)\n",
        "        # Get the counterfactuals\n",
        "        # find counterfactuals\n",
        "        counter_factuals = factuals.apply(\n",
        "            lambda x: self.tree_based_search(x), axis=1, raw=False\n",
        "        )\n",
        "        # counter_factuals = [self.tree_based_search(row) for __,row in factuals.iterrows()]\n",
        "        # Concatenate the counterfactuals to a single dataframe\n",
        "        # counter_factuals is a list of rows\n",
        "        self.counter_factuals = counter_factuals\n",
        "        #counter_factuals = check_counterfactuals(self._mlmodel, counter_factuals)\n",
        "        # Return the counterfactuals\n",
        "        return counter_factuals[self._mlmodel.feature_input_order]\n",
        "\n",
        "    def get_encodeings(self, data: pd.DataFrame):\n",
        "        '''\n",
        "        This method is responsible to append the encoded features\n",
        "        to the dataframe\n",
        "        '''\n",
        "        # Fix DataFrame to be able to feed to the VAE\n",
        "        input_data = data.copy()[self._mlmodel.feature_input_order]\n",
        "        input_data = torch.FloatTensor(input_data.values)\n",
        "        # Get the encoded features\n",
        "        encoded_values = self.vae.encode(input_data)[0].detach().numpy()\n",
        "        encoded_values = [i for i in encoded_values]\n",
        "        return encoded_values\n",
        "\n",
        "    def distance_get(self, x,factuals):\n",
        "        return np.square((x - factuals)).sum()\n",
        "\n",
        "    def get_nearest_neighbors_thershold(self, copy_data, label_threshold):\n",
        "        '''\n",
        "        This method is responsible to get the nearest neighbors of a given threshold\n",
        "        using the VAE and minimum threshold per label\n",
        "        '''\n",
        "        # Find the index of the 100th instance of each class\n",
        "        id_100th_class_0 = copy_data[copy_data[self._mlmodel.data.target] == 0].index[label_threshold-1]\n",
        "        id_100th_class_1 = copy_data[copy_data[self._mlmodel.data.target] == 1].index[label_threshold-1]\n",
        "        # Get the maximum id\n",
        "        max_id = max(id_100th_class_0, id_100th_class_1)\n",
        "        # Return the nearest neighbors of the 100th instance of each class\n",
        "        return copy_data.head(max_id)\n",
        "    \n",
        "    def decision_tree(self, nearest_neighbors):\n",
        "        '''\n",
        "        This method is responsible to create a decision tree\n",
        "        using the nearest neighbors of the 100th instance of each class\n",
        "        '''\n",
        "        target_values = nearest_neighbors[self._mlmodel.data.target]\n",
        "        train_features = nearest_neighbors[self._mlmodel.feature_input_order]\n",
        "        # Create the decision tree\n",
        "        clf = DecisionTreeClassifier(random_state=0 , max_depth=self.hyperparams[\"tree_params\"]['grid_search'][\"max_depth\"], \n",
        "                                    min_samples_split=self.hyperparams[\"tree_params\"]['grid_search'][\"min_samples_split\"], \n",
        "                                    min_samples_leaf=self.hyperparams[\"tree_params\"]['grid_search'][\"min_samples_leaf\"], \n",
        "                                    max_features=self.hyperparams[\"tree_params\"]['grid_search'][\"max_features\"])\n",
        "        # Define the grid search\n",
        "        #grid_search = GridSearchCV(clf, self.hyperparams[\"tree_params\"][\"grid_search\"], cv=5, verbose=0, refit=True, n_jobs=self.hyperparams[\"tree_params\"][\"grid_search_jobs\"])\n",
        "        # Fit the grid search evaluate on X_test and y_test then refit best model on the whole dataset\n",
        "        #grid_search.fit(train_features, target_values)\n",
        "        # Return the best model\n",
        "        #return grid_search.best_estimator_\n",
        "        clf.fit(train_features, target_values)\n",
        "        return clf\n",
        "\n",
        "    def tree_based_search(self, factual):\n",
        "        '''\n",
        "        This method is responsible to get the counterfactual of a given targeted_encoding\n",
        "        '''\n",
        "        copy_data = self.dataset.copy()\n",
        "        ## Get distances from data to this encoding\n",
        "        #time1 = time.time()\n",
        "        #copy_data[\"distance\"] = copy_data[\"VAE_ENCODED\"].apply(lambda x: self.distance_get(x, factual[\"VAE_ENCODED\"]))\n",
        "        #print(\"Old Method Took: {:.3f} seconds\".format(time.time()-time1))\n",
        "        ### Use the KD tree to query the 1000 neighbors of data_indexes_1 from factual[\"VAE_ENCODED\"]\n",
        "        ##TODO SHIFT TO pynndescent\n",
        "        #index_neighbors_1 = self.kdtree_1.query(factual[\"VAE_ENCODED\"], k=self.hyperparams[\"tree_params\"][\"min_entries_per_label\"])[1]\n",
        "        ### index_neighbors_1 are the locations of indexes in self.data_indexes_1\n",
        "        ### Use index_neighbors_1 to get the data_indexes_1\n",
        "        #datata_index_1 = self.data_indexes_1[index_neighbors_1].tolist()\n",
        "        #index_neighbors_0 = self.kdtree_0.query(factual[\"VAE_ENCODED\"], k=self.hyperparams[\"tree_params\"][\"min_entries_per_label\"])[1]\n",
        "        #datata_index_0 = self.data_indexes_0[index_neighbors_0].tolist()\n",
        "        ### combine the data_indexes_1 and data_indexes_0 lists\n",
        "        #datata_index_0.extend(datata_index_1)\n",
        "        ## Get the data from the indexes\n",
        "        # MAIN KDTREE\n",
        "        #index_neighbors_0 = self.kdtree_m.query(factual[\"VAE_ENCODED\"], k=self.hyperparams[\"tree_params\"][\"min_entries_per_label\"]*2.5)[1].tolist()\n",
        "        # NNDescent with manhatan\n",
        "        index_neighbors_0 = self.nnd.query(np.array([factual[\"VAE_ENCODED\"].tolist()]), k=self.hyperparams[\"tree_params\"][\"min_entries_per_label\"]*2.5)[0][0].tolist()\n",
        "        datata_index_0 = self.data_indexes_m[index_neighbors_0].tolist()\n",
        "        nearest_neighbors = copy_data.loc[datata_index_0]\n",
        "        #datata_index_1 = np.random.choice(self.data_indexes_1, self.hyperparams[\"tree_params\"][\"min_entries_per_label\"], replace=False).tolist()\n",
        "        #datata_index_0 = np.random.choice(self.data_indexes_0, self.hyperparams[\"tree_params\"][\"min_entries_per_label\"], replace=False).tolist()\n",
        "        #datata_index_0.extend(datata_index_1)\n",
        "        #nearest_neighbors = copy_data.loc[datata_index_0]        \n",
        "        ## Sort the dataframe by distance\n",
        "        #time1 = time.time()\n",
        "        #copy_data = copy_data.sort_values(by=\"distance\")\n",
        "        #print(\"Sort Took: {:.3f} seconds\".format(time.time()-time1))\n",
        "        ## Reset the index\n",
        "        #copy_data = copy_data.reset_index(drop=True)\n",
        "        ## Get the nearest neighbors of the targeted encoding\n",
        "        #time1 = time.time()\n",
        "        #nearest_neighbors = self.get_nearest_neighbors_thershold(copy_data, label_threshold=self.hyperparams[\"tree_params\"][\"min_entries_per_label\"])\n",
        "        #print(\"Get Neighbors Took: {:.3f} seconds\".format(time.time()-time1))\n",
        "        # Get the tree\n",
        "        tree = self.decision_tree(nearest_neighbors)\n",
        "        self.mtree = tree\n",
        "        # Get the leaf nodes\n",
        "        leaf_nodes = TreeLeafs(tree.tree_, self._mlmodel.feature_input_order).leafs_nodes.copy()\n",
        "        # leaf_nodes is list of classes LeafNode\n",
        "        # Get the leaf node where the targeted encoding is located\n",
        "        leaf_node_n_i = -1\n",
        "        for leaf_i in range(len(leaf_nodes)):\n",
        "            if leaf_nodes[leaf_i].check_point(factual):\n",
        "                leaf_node_n_i = leaf_i\n",
        "                break\n",
        "        self.mleaf_node_n_i = leaf_node_n_i\n",
        "        self.mfactual = factual\n",
        "        # assert if the leaf node is not found\n",
        "        assert leaf_node_n_i != -1, \"Leaf node not found\"\n",
        "        # For now change leafnode label to the item label\n",
        "        # #assert if leaf_node_n.label is not the same as the factual label\n",
        "        # assert leaf_node_n.label == factual[self._mlmodel.data.target], \"Leaf node label {} is not the same as the factual label {}\".format(leaf_node_n.label, factual[self._mlmodel.data.target])\n",
        "        if leaf_nodes[leaf_node_n_i].label != factual[self._mlmodel.data.target]:\n",
        "          #print(\"Leaf Node {} flipped node label {} to match the factual entry {}\".format(leaf_node_n_i,\n",
        "          #                                                                          leaf_nodes[leaf_node_n_i].label,\n",
        "          #                                                                          factual[self._mlmodel.data.target]))\n",
        "          leaf_nodes[leaf_node_n_i].label = factual[self._mlmodel.data.target]\n",
        "        leaf_node_n = leaf_nodes[leaf_node_n_i]\n",
        "        # Get all leafnodes with label!= leaf_node_n.label and Sort leaf nodes by distance\n",
        "        leaf_nodes_with_label = [leaf_n for leaf_n in leaf_nodes if leaf_n.label != leaf_node_n.label]\n",
        "        # Check if leaf_nodes_with_label is empty\n",
        "        if len(leaf_nodes_with_label) == 0:\n",
        "            print(\"No leaf node with label {}\".format(leaf_node_n.label))\n",
        "            factual_ret = factual\n",
        "            factual_ret[self._mlmodel.feature_input_order] = np.nan\n",
        "            #print(\"returned\")\n",
        "            return factual_ret[self._mlmodel.feature_input_order]\n",
        "        # Sort leaf nodes by distance\n",
        "        leaf_nodes_with_label = sorted(leaf_nodes_with_label, key=lambda x: len(leaf_node_n.compare_node(x)))\n",
        "        # Get the counterfactual\n",
        "        returned_neighbor = -1\n",
        "        counter_taregt = factual[self._mlmodel.data.target]*-1 +1\n",
        "        #print(\"Searching for Neighbor....\")\n",
        "        # print(\"Start with option A: {}\".format(nearest_leaf_node))\n",
        "        #print(second_nearest_node)\n",
        "        # If len of leaf_nodes_with_label is 1, the all the max_search on the nearest_leaf_node\n",
        "        # If len of leaf_nodes_with_label is 2, the max_search/7 on the nearest_leaf_node and max_search/3 on the second_nearest_node\n",
        "        # If len of leaf_nodes_with_label is 3, the max_search/5 on the nearest_leaf_node and max_search/3 on the second_nearest_node and max_search/2 on the third_nearest_node\n",
        "        if len(leaf_nodes_with_label) == 1:\n",
        "            max_searchs = [self.hyperparams[\"tree_params\"][\"max_search\"]]\n",
        "        elif len(leaf_nodes_with_label) == 2:\n",
        "            max_searchs = [self.hyperparams[\"tree_params\"][\"max_search\"]*0.8, self.hyperparams[\"tree_params\"][\"max_search\"]*0.2]\n",
        "        else:\n",
        "            max_searchs = [self.hyperparams[\"tree_params\"][\"max_search\"]*0.7, self.hyperparams[\"tree_params\"][\"max_search\"]*0.2, self.hyperparams[\"tree_params\"][\"max_search\"]*0.1]\n",
        "        # map max_search to int values while rounding up to the nearest int\n",
        "        max_searchs = [int(round(x)) for x in max_searchs]\n",
        "        # Loop over max_search\n",
        "        for rank_node, max_search_i in enumerate(max_searchs):\n",
        "            number_searchs = 0\n",
        "            nearest_leaf_node = leaf_nodes_with_label[rank_node]\n",
        "            #print(\"Searching for Neighbor.... {}, {}\".format(rank_node, max_search_i))\n",
        "            while number_searchs < max_search_i and returned_neighbor is -1:\n",
        "                # if number_searchs is 30% of max_search\n",
        "                if number_searchs < max_search_i*0.5:\n",
        "                    sigma = 10\n",
        "                    gamma = 0\n",
        "                # if number_searchs is 80% of max_search\n",
        "                elif number_searchs < max_search_i*0.8:\n",
        "                    sigma = 0.25\n",
        "                    gamma = 0\n",
        "                # if number_searchs is 80% of max_search\n",
        "                elif number_searchs < max_search_i*0.9:\n",
        "                    sigma = 1\n",
        "                    gamma = 10\n",
        "                # if number_searchs is 90% of max_search\n",
        "                else:\n",
        "                    sigma = 0.2\n",
        "                    gamma = 1\n",
        "                neighbor = nearest_leaf_node.generate_point(factual.copy(), data_catalog = self.data_catalog, sigma = sigma, gamma = gamma)\n",
        "                neighb_df = pd.DataFrame([neighbor[self.mlmodel.feature_input_order]])\n",
        "                self.neighb_df = neighb_df\n",
        "                probs_p = self.mlmodel.predict_proba(neighb_df)\n",
        "                if counter_taregt == np.argmax(probs_p):\n",
        "                    returned_neighbor = neighbor\n",
        "                    break\n",
        "                number_searchs += 1\n",
        "            if returned_neighbor is not -1:\n",
        "                break\n",
        "        # If no neighbor is found, return the factual\n",
        "        if returned_neighbor is -1:\n",
        "            #print(\"No neighbor was found\")\n",
        "            factual_ret = factual\n",
        "            factual_ret[self._mlmodel.feature_input_order] = np.nan\n",
        "            #print(\"returned\")\n",
        "            return factual_ret[self._mlmodel.feature_input_order]\n",
        "        return returned_neighbor[self._mlmodel.feature_input_order]\n"
      ],
      "metadata": {
        "id": "ImMBjT2iQPWk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from carla import MLModelCatalog\n",
        "from carla.data.catalog import OnlineCatalog\n",
        "from carla.recourse_methods import GrowingSpheres\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "# load a catalog dataset\n",
        "data_name = \"compas\"\n",
        "dataset = OnlineCatalog(data_name)\n",
        "data_train, data_test = train_test_split(dataset.df, test_size=0.2)\n",
        "\n",
        "class MyData:\n",
        "  def __init__(self, data, target):\n",
        "    self.df = data\n",
        "    self.target = target\n",
        "trainData = MyData(data_train.copy(), dataset.target)\n",
        "\n",
        "# load artificial neural network from catalog\n",
        "model = MLModelCatalog(dataset, 'ann')\n",
        "factuals = data_test.head(50)"
      ],
      "metadata": {
        "id": "V4WllcD0t3t6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check our data catalog\n",
        "col_n = dataset.df.columns\n",
        "catalog_n = dataset.catalog\n",
        "# Initialize new catalog\n",
        "new_catalog_n = {'target': model.data.target, 'continuous': [], 'categorical': [], 'immutable': []}\n",
        "# Map continuous values\n",
        "for col_i in col_n:\n",
        "    chk_col = True\n",
        "    if col_i in dataset.target:\n",
        "        continue\n",
        "    for col_j in catalog_n['immutable']:\n",
        "        if col_j in col_i:\n",
        "            new_catalog_n['immutable'].append(col_i)\n",
        "            break\n",
        "    for col_j in catalog_n['continuous']:\n",
        "        if col_j in col_i:\n",
        "            new_catalog_n['continuous'].append(col_i)\n",
        "            chk_col = False\n",
        "            break\n",
        "    for col_j in catalog_n['categorical']:\n",
        "        if col_j in col_i:\n",
        "            new_catalog_n['categorical'].append(col_i)\n",
        "            chk_col = False\n",
        "            break\n",
        "    if chk_col:\n",
        "        assert False, 'Column not found in catalog {}'.format(col_i)\n",
        "\n",
        "# Assert if new_catalog_n is not same shape as catalog_n\n",
        "assert len(new_catalog_n['continuous']) == len(catalog_n['continuous']), 'Continuous values not same shape'\n",
        "assert len(new_catalog_n['categorical']) == len(catalog_n['categorical']), 'Categorical values not same shape'\n",
        "assert len(new_catalog_n['immutable']) == len(catalog_n['immutable']), 'Immutable values not same shape'\n",
        "# For each continous value get the std, mean, and min/max and plug them in the new catalog['continuous_stats']\n",
        "new_catalog_n['continuous_stats'] = {}\n",
        "for col_i in new_catalog_n['continuous']:\n",
        "    new_catalog_n['continuous_stats'][col_i] = {}\n",
        "    new_catalog_n['continuous_stats'][col_i]['std'] = data_train[col_i].std()\n",
        "    new_catalog_n['continuous_stats'][col_i]['mean'] = data_train[col_i].mean()\n",
        "    new_catalog_n['continuous_stats'][col_i]['min'] = data_train[col_i].min()\n",
        "    new_catalog_n['continuous_stats'][col_i]['max'] = data_train[col_i].max()\n"
      ],
      "metadata": {
        "id": "hm9OzZqJd8kw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def score_acc(model):\n",
        "    dataset = model.data\n",
        "    data = dataset.df\n",
        "    input_cols = model.feature_input_order\n",
        "    target_column = dataset.target\n",
        "    X = data[input_cols]\n",
        "    y = data[target_column]\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "SSwy1i4_F9d6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from carla.models.catalog import MLModelCatalog\n",
        "seed_my_session()\n",
        "with contextlib.redirect_stdout(open('models_logs.txt', 'w')):\n",
        "  training_params_ann = {\"lr\": 0.002, \"epochs\": 10, \"batch_size\": 128, \"hidden_size\": [13,4]}\n",
        "  #training_params_ann = {\"lr\": 0.001, \"epochs\": 3, \"batch_size\": 64, \"hidden_size\": [18, 9, 3]}\n",
        "  tf_ann_model = MLModelCatalog(\n",
        "      dataset,\n",
        "      model_type=\"ann\",\n",
        "      load_online=False,\n",
        "      backend=\"tensorflow\"\n",
        "  )\n",
        "\n",
        "  tf_ann_model.train(\n",
        "      learning_rate=training_params_ann[\"lr\"],\n",
        "      epochs=training_params_ann[\"epochs\"],\n",
        "      batch_size=training_params_ann[\"batch_size\"],\n",
        "      hidden_size=training_params_ann[\"hidden_size\"]\n",
        "  )\n",
        "  nn_ann_model = MLModelCatalog(\n",
        "      dataset,\n",
        "      model_type=\"ann\",\n",
        "      load_online=False,\n",
        "      backend=\"pytorch\"\n",
        "  )\n",
        "\n",
        "  nn_ann_model.train(\n",
        "      learning_rate=training_params_ann[\"lr\"],\n",
        "      epochs=training_params_ann[\"epochs\"],\n",
        "      batch_size=training_params_ann[\"batch_size\"],\n",
        "      hidden_size=training_params_ann[\"hidden_size\"]\n",
        "  )\n",
        "  training_params_linear = {\"lr\": 0.002, \"epochs\": 50, \"batch_size\": 128, \"hidden_size\": [13,4]}\n",
        "  #training_params_ann = {\"lr\": 0.001, \"epochs\": 3, \"batch_size\": 64, \"hidden_size\": [18, 9, 3]}\n",
        "\n",
        "  tf_linear_model = MLModelCatalog(\n",
        "      dataset,\n",
        "      model_type=\"linear\",\n",
        "      load_online=False,\n",
        "      backend=\"tensorflow\"\n",
        "  )\n",
        "\n",
        "  tf_linear_model.train(\n",
        "      learning_rate=training_params_linear[\"lr\"],\n",
        "      epochs=training_params_linear[\"epochs\"],\n",
        "      batch_size=training_params_linear[\"batch_size\"]\n",
        "  )\n",
        "  nn_linear_model = MLModelCatalog(\n",
        "      dataset,\n",
        "      model_type=\"linear\",\n",
        "      load_online=False,\n",
        "      backend=\"pytorch\"\n",
        "  )\n",
        "\n",
        "  nn_linear_model.train(\n",
        "      learning_rate=training_params_linear[\"lr\"],\n",
        "      epochs=training_params_linear[\"epochs\"],\n",
        "      batch_size=training_params_linear[\"batch_size\"]\n",
        "  )\n",
        "  training_params_tree = {\"max_depth\": 2, \"n_estimators\": 5}\n",
        "\n",
        "  xg_tree_model = MLModelCatalog(\n",
        "      dataset,\n",
        "      model_type=\"forest\",\n",
        "      load_online=False,\n",
        "      backend=\"xgboost\"\n",
        "  )\n",
        "\n",
        "  xg_tree_model.train(\n",
        "      max_depth=training_params_tree[\"max_depth\"],\n",
        "      n_estimators=training_params_tree[\"n_estimators\"]\n",
        "  )\n",
        "  sk_tree_model = MLModelCatalog(\n",
        "      dataset,\n",
        "      model_type=\"forest\",\n",
        "      load_online=False,\n",
        "      backend=\"sklearn\"\n",
        "  )\n",
        "\n",
        "  sk_tree_model.train(\n",
        "      max_depth=training_params_tree[\"max_depth\"],\n",
        "      n_estimators=training_params_tree[\"n_estimators\"]\n",
        "  )\n",
        "print(\"Forests:\\n\\tXGboost Accuracy: {:.2f}\".format(score_acc(xg_tree_model)))\n",
        "print(\"\\tSklearn Accuracy: {:.2f}\".format(score_acc(sk_tree_model)))\n",
        "print(\"ANN:\\n\\tTensrf. Accuracy: {:.2f}\".format(score_acc(tf_ann_model)))\n",
        "print(\"\\tPytorch Accuracy: {:.2f}\".format(score_acc(nn_ann_model)))\n",
        "print(\"Linear:\\n\\tTensrf. Accuracy: {:.2f}\".format(score_acc(tf_linear_model)))\n",
        "print(\"\\tPytorch Accuracy: {:.2f}\".format(score_acc(nn_linear_model)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEOQCVR0AQqs",
        "outputId": "6d993c35-ec10-45f2-cc46-3f5a83ca7191"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forests:\n",
            "\tXGboost Accuracy: 0.84\n",
            "\tSklearn Accuracy: 0.83\n",
            "ANN:\n",
            "\tTensrf. Accuracy: 0.83\n",
            "\tPytorch Accuracy: 0.84\n",
            "Linear:\n",
            "\tTensrf. Accuracy: 0.84\n",
            "\tPytorch Accuracy: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from carla.recourse_methods import *"
      ],
      "metadata": {
        "id": "q8ET308R3C4H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import KDTree\n",
        "import scipy"
      ],
      "metadata": {
        "id": "JvIn4SNV3ygz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install pynndescent'''"
      ],
      "metadata": {
        "id": "dymB-afG5OPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install scipy==1.4.1\n",
        "!pip install scikit_learn==1.4.1\n",
        "!pip install setuptools==57.4.0'''"
      ],
      "metadata": {
        "id": "NvrfZF0B-1rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install -r /content/CARLA/requirements-dev.txt'''"
      ],
      "metadata": {
        "id": "o4lOu9b-_zof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pynndescent import NNDescent"
      ],
      "metadata": {
        "id": "jcjESBZ2xENv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP_VAE = tbtest.vae"
      ],
      "metadata": {
        "id": "LIL1UsLWUr3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_data = data_test.sample(200)"
      ],
      "metadata": {
        "id": "lDvrI5gzESlE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hpr = {\n",
        "      \"data_name\": \"data_name\",\n",
        "      \"n_search_samples\": 300,\n",
        "      \"p_norm\": 1,\n",
        "      \"step\": 0.1,\n",
        "      \"max_iter\": 10,\n",
        "      \"clamp\": True,\n",
        "      \"binary_cat_features\": True,\n",
        "      \"vae_params\": {\n",
        "          \"layers\": [len(model.feature_input_order), 20, 10, 7],\n",
        "          \"train\": True,\n",
        "          \"lambda_reg\": 1e-6,\n",
        "          \"epochs\": 5,\n",
        "          \"lr\": 1e-3,\n",
        "          \"batch_size\": 16,\n",
        "      },\n",
        "      \"tree_params\": {\n",
        "          \"min_entries_per_label\": int(trainData.df.shape[0]*0.04),\n",
        "          \"grid_search_jobs\": -1,\n",
        "          \"min_weight_gini\": 100, # set to 0.5 since here both class have same prob,\n",
        "          \"max_search\" : 150,\n",
        "          \"grid_search\": {\n",
        "                \"splitter\": [\"best\"],\n",
        "                \"criterion\": [\"gini\"],\n",
        "                \"max_depth\": [3,4,5,6,7,8,9,10],\n",
        "                \"min_samples_split\": [1,2,3],\n",
        "                \"min_samples_leaf\": [1,2,3],\n",
        "                \"max_features\": [None] #Note changing this will result in removing features that we might want to keep\n",
        "          }\n",
        "      }\n",
        "    }\n",
        "\n",
        "# Conditions Violations add it at the beginning before going to sampling\n",
        "# Immutable, e.g. gender imutable all directions\n",
        "# Age should be greater than x okay, bs decrease \n",
        "# Our method take care of those imutability\n",
        "# \n",
        "# Decision tree validation train \n",
        "# DOn't cross. validation\n",
        "# Don't retrain\n",
        "# Just to retrain\n",
        "#julia here\n",
        "tbtest = TreeBasedContrastiveExplanation(trainData, tf_ann_model, hpr, data_catalog= new_catalog_n)\n",
        "#skk = tbtest.get_counterfactuals(factuals.copy().head(10))\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, factuals.copy().head(5).copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "benchmark.run_benchmark().head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "MFWEcoPYzNZm",
        "outputId": "bfc7acbe-1da7-4079-b62c-b51fa5feaf58"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Start training of Variational Autoencoder... [models.py fit]\n",
            "[INFO] [Epoch: 0/5] [objective: 2.404] [models.py fit]\n",
            "[INFO] [ELBO train: 2.4] [models.py fit]\n",
            "[INFO] [ELBO train: 0.37] [models.py fit]\n",
            "[INFO] [ELBO train: 0.24] [models.py fit]\n",
            "[INFO] [ELBO train: 0.2] [models.py fit]\n",
            "[INFO] [ELBO train: 0.17] [models.py fit]\n",
            "[INFO] ... finished training of Variational Autoencoder. [models.py fit]\n",
            "{'criterion': 'gini', 'max_depth': 4, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Distance_1  Distance_2  Distance_3  Distance_4  Constraint_Violation  \\\n",
              "0         2.0     0.11869    0.008906    0.089857                     1   \n",
              "\n",
              "   Redundancy  y-Nearest-Neighbours  Success_Rate  Average_Time  \n",
              "0           2                   1.0           0.2      0.671396  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-002a1511-e485-4113-9320-926634bb329e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Distance_1</th>\n",
              "      <th>Distance_2</th>\n",
              "      <th>Distance_3</th>\n",
              "      <th>Distance_4</th>\n",
              "      <th>Constraint_Violation</th>\n",
              "      <th>Redundancy</th>\n",
              "      <th>y-Nearest-Neighbours</th>\n",
              "      <th>Success_Rate</th>\n",
              "      <th>Average_Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.11869</td>\n",
              "      <td>0.008906</td>\n",
              "      <td>0.089857</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.671396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-002a1511-e485-4113-9320-926634bb329e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-002a1511-e485-4113-9320-926634bb329e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-002a1511-e485-4113-9320-926634bb329e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_scores = {}"
      ],
      "metadata": {
        "id": "yc1FpOc3Krfd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.hyperparams[\"tree_params\"][\"max_search\"] = 150\n",
        "tbtest.hyperparams[\"tree_params\"][\"min_entries_per_label\"] = int(trainData.df.shape[0]*0.04)"
      ],
      "metadata": {
        "id": "fhGWem1DE10q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.set_distance_metric_initialize_nn('hamming')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['hamming'] = benchmark.run_benchmark().mean()\n",
        "metrics_scores['hamming']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB-ROYxhF3Sn",
        "outputId": "9b419d8e-40e3-4a58-8f7c-819d84e47497"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Distance_1              2.481928\n",
              "Distance_2              1.344230\n",
              "Distance_3              1.076085\n",
              "Distance_4              0.816193\n",
              "Constraint_Violation    1.036145\n",
              "Redundancy              1.963855\n",
              "y-Nearest-Neighbours    0.766265\n",
              "Success_Rate            0.415000\n",
              "Average_Time            0.331683\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.set_distance_metric_initialize_nn('euclidean')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['euclidean'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['euclidean']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Zi6EyXr-HBny",
        "outputId": "5722396a-7b65-4fca-b68e-ab2e9383a163"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No leaf node with label 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Distance_1  Distance_2  Distance_3  Distance_4  Constraint_Violation  \\\n",
              "0         2.0    0.446827    0.101851    0.255227                     1   \n",
              "\n",
              "   Redundancy  y-Nearest-Neighbours  Success_Rate  Average_Time  \n",
              "0           2              0.849057         0.265      0.347757  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c9c6ec2-a9a1-461c-9413-0cfb00ba8857\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Distance_1</th>\n",
              "      <th>Distance_2</th>\n",
              "      <th>Distance_3</th>\n",
              "      <th>Distance_4</th>\n",
              "      <th>Constraint_Violation</th>\n",
              "      <th>Redundancy</th>\n",
              "      <th>y-Nearest-Neighbours</th>\n",
              "      <th>Success_Rate</th>\n",
              "      <th>Average_Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.446827</td>\n",
              "      <td>0.101851</td>\n",
              "      <td>0.255227</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.849057</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.347757</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c9c6ec2-a9a1-461c-9413-0cfb00ba8857')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c9c6ec2-a9a1-461c-9413-0cfb00ba8857 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c9c6ec2-a9a1-461c-9413-0cfb00ba8857');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.set_distance_metric_initialize_nn('euclidean')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "benchmark.run_benchmark().head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "B_qJ88o6FJ0V",
        "outputId": "ce068454-f9c3-421f-b33d-2d9763e8c29b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No leaf node with label 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Distance_1  Distance_2  Distance_3  Distance_4  Constraint_Violation  \\\n",
              "0         2.0    0.449209    0.103347     0.25962                     1   \n",
              "\n",
              "   Redundancy  y-Nearest-Neighbours  Success_Rate  Average_Time  \n",
              "0           2              0.856604         0.265      0.450061  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b2076ab-9dbb-4ac8-ae29-38525097d39f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Distance_1</th>\n",
              "      <th>Distance_2</th>\n",
              "      <th>Distance_3</th>\n",
              "      <th>Distance_4</th>\n",
              "      <th>Constraint_Violation</th>\n",
              "      <th>Redundancy</th>\n",
              "      <th>y-Nearest-Neighbours</th>\n",
              "      <th>Success_Rate</th>\n",
              "      <th>Average_Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.449209</td>\n",
              "      <td>0.103347</td>\n",
              "      <td>0.25962</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.856604</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.450061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b2076ab-9dbb-4ac8-ae29-38525097d39f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b2076ab-9dbb-4ac8-ae29-38525097d39f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b2076ab-9dbb-4ac8-ae29-38525097d39f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.set_distance_metric_initialize_nn('manhattan')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['manhattan'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['manhattan']"
      ],
      "metadata": {
        "id": "rFQDo2FzDhZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.set_distance_metric_initialize_nn('cosine')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['cosine'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['cosine']"
      ],
      "metadata": {
        "id": "liLSxQ4DDjFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.set_distance_metric_initialize_nn('correlation')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['correlation'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['correlation']"
      ],
      "metadata": {
        "id": "BtK1bw6rISpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.set_distance_metric_initialize_nn('dice')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['dice'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['dice']"
      ],
      "metadata": {
        "id": "4rRS0nKuIYNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs = GrowingSpheres(tf_ann_model)\n",
        "benchmark = Benchmark(tf_ann_model, gs,  check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['GrowingSpheres'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['GrowingSpheres']"
      ],
      "metadata": {
        "id": "Dvyh7-VILEIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = setup[\"dice\"][\"hyperparams\"]\n",
        "dice = Dice(model, hyperparams)\n",
        "benchmark = Benchmark(model, dice,  check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['DiceResource'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['DiceResource']"
      ],
      "metadata": {
        "id": "l9xfJFzQLEjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbtest.hyperparams[\"tree_params\"][\"max_search\"] = 10\n",
        "tbtest.set_distance_metric_initialize_nn('hamming')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['hammingxspeed'] = benchmark.run_benchmark().head(1)\n",
        "tbtest.set_distance_metric_initialize_nn('euclidean')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['euclideanxspeed'] = benchmark.run_benchmark().head(1)\n",
        "tbtest.set_distance_metric_initialize_nn('manhattan')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['manhattanxspeed'] = benchmark.run_benchmark().head(1)\n",
        "tbtest.set_distance_metric_initialize_nn('cosine')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['cosinexspeed'] = benchmark.run_benchmark().head(1)\n",
        "tbtest.set_distance_metric_initialize_nn('correlation')\n",
        "benchmark = Benchmark(tf_ann_model, tbtest, check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['correlationxspeed'] = benchmark.run_benchmark().head(1)"
      ],
      "metadata": {
        "id": "WaxV36B8SHWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dataFrame = pd.DataFrame(metrics_scores).transpose()\n",
        "scores_dataFrame.to_csv('compas_scores_dataFrame.csv')"
      ],
      "metadata": {
        "id": "cM5x_pMrLEvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from carla.recourse_methods import CCHVAE\n",
        "hyperparams = setup[\"cchvae\"][\"hyperparams\"]\n",
        "hyperparams[\"data_name\"] = data_name\n",
        "hyperparams[\"vae_params\"][\"layers\"] = [\n",
        "    len(nn_ann_model.feature_input_order)\n",
        "] + hyperparams[\"vae_params\"][\"layers\"]\n",
        "cchvae = CCHVAE(nn_ann_model, hyperparams)\n",
        "metrics_scores['CCHVAE'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['CCHVAE']"
      ],
      "metadata": {
        "id": "olf71t-V-MFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dataFrame = pd.DataFrame(metrics_scores).transpose()\n",
        "scores_dataFrame.to_csv('compas_scores_dataFrame.csv')"
      ],
      "metadata": {
        "id": "hC-JwQCTWGoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from carla.recourse_methods import CRUD\n",
        "hyperparams = setup[\"cruds\"][\"hyperparams\"]\n",
        "hyperparams[\"data_name\"] = data_name\n",
        "hyperparams[\"vae_params\"][\"layers\"] = [\n",
        "    len(nn_ann_model.feature_input_order)\n",
        "] + hyperparams[\"vae_params\"][\"layers\"]\n",
        "cruds = CRUD(nn_ann_model, hyperparams)\n",
        "benchmark = Benchmark(nn_ann_model, cruds,  check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['CRUD'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['CRUD']"
      ],
      "metadata": {
        "id": "ApLt8KD6-3db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dataFrame = pd.DataFrame(metrics_scores).transpose()\n",
        "scores_dataFrame.to_csv('compas_scores_dataFrame.csv')"
      ],
      "metadata": {
        "id": "McIkQW5GWTFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from carla.recourse_methods import Wachter\n",
        "hyperparams = setup[\"wachter\"][\"hyperparams\"]\n",
        "wachter = Wachter(nn_ann_model, hyperparams)\n",
        "benchmark = Benchmark(nn_ann_model, wachter,  check_data.copy().reset_index(drop=True))\n",
        "distances = benchmark.compute_distances()\n",
        "metrics_scores['Wachter'] = benchmark.run_benchmark().head(1)\n",
        "metrics_scores['Wachter']"
      ],
      "metadata": {
        "id": "j7kJTFLF-3qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dataFrame = pd.DataFrame(metrics_scores).transpose()\n",
        "scores_dataFrame.to_csv('compas_scores_dataFrame.csv')"
      ],
      "metadata": {
        "id": "biA7eQITWXZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from carla.recourse_methods import Revise\n",
        "hyperparams = setup[\"revise\"][\"hyperparams\"]\n",
        "hyperparams[\"data_name\"] = data_name\n",
        "hyperparams[\"vae_params\"][\"layers\"] = [\n",
        "    len(nn_ann_model.feature_input_order)\n",
        "] + hyperparams[\"vae_params\"][\"layers\"]\n",
        "revise = Revise(nn_ann_model, trainData, hyperparams)\n",
        "benchmark = Benchmark(nn_ann_model, revise,  check_data.copy().reset_index(drop=True))\n",
        "metrics_scores['Revise'] = benchmark.run_benchmark().mean()\n",
        "metrics_scores['Revise']"
      ],
      "metadata": {
        "id": "eQa_ag6j-3lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dataFrame = pd.DataFrame(metrics_scores).transpose()\n",
        "scores_dataFrame.to_csv('compas_scores_dataFrame.csv')"
      ],
      "metadata": {
        "id": "13PmcEL_Ynsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "58eZHnZC-3y4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}